{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle8k_randomForest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0CP5gNRYs6u4",
        "colab_type": "code",
        "outputId": "85dfdca8-c20e-4433-fc95-bd0ff9f4b095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following code is run on google cloud vm due to the memory insufficient error we encountered on colab.\n",
        "# For result please refer to the report.\n",
        "# \n",
        "##### loading data #####\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VXntJ-CntCa8",
        "colab_type": "code",
        "outputId": "a1d90e37-fab7-4d6f-f9f8-6faab263a118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "train_dir = \"/gdrive/My Drive/kaggle_dataset/train/\"\n",
        "val_dir = \"/gdrive/My Drive/kaggle_dataset/val/\"\n",
        "test_dir = \"/gdrive/My Drive/kaggle_dataset/test/\"\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "\n",
        "# img_width = 512\n",
        "# img_height = 496\n",
        "\n",
        "batch_size = 128\n",
        "channels = 3\n",
        "epochs = 50\n",
        "nb_train_samples = 8000\n",
        "nb_valid_samples = 32\n",
        "nb_test_samples = 968\n",
        "num_classes = 4\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)    \n",
        "test_datagen = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, \n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True)   \n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True) #weight toward one class or another\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n",
            "Found 968 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQznwHqmtFcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def stacking_samples(dataset_type, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, img_height, img_width, channels))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    i = 0\n",
        "    if dataset_type == \"train\":\n",
        "        for inputs_batch, labels_batch in train_generator:\n",
        "#             features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break   \n",
        "    elif dataset_type == \"valid\":\n",
        "        for inputs_batch, labels_batch in valid_generator:\n",
        "#             features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    else:\n",
        "        for inputs_batch, labels_batch in test_generator:\n",
        "#             features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = stacking_samples(\"train\", nb_train_samples)\n",
        "valid_features, valid_labels = stacking_samples(\"valid\", nb_valid_samples)\n",
        "test_features, test_labels = stacking_samples(\"test\", nb_test_samples)\n",
        "\n",
        "\n",
        "print(train_features.shape, train_labels.shape)\n",
        "print(valid_features.shape, valid_labels.shape)\n",
        "print(test_features.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SZn06L4tPZM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=12, random_state=0, bootstrap=True)\n",
        "clf.fit(np.reshape(train_features, (nb_train_samples,img_height*img_width*channels)), train_labels)\n",
        "\n",
        "valid_prediction = clf.predict(np.reshape(valid_features, (nb_valid_samples, img_height*img_width*channels)))\n",
        "test_prediction = clf.predict(np.reshape(test_features, (nb_test_samples, img_height*img_width*channels))) \n",
        "\n",
        "valid_score = clf.score(np.reshape(valid_features, (nb_valid_samples, img_height*img_width*channels)), valid_labels)\n",
        "test_score = clf.score(np.reshape(test_features, (nb_test_samples, img_height*img_width*channels)), test_labels)\n",
        "\n",
        "print(\"valid accuracy:\", valid_score)\n",
        "print(\"test accuracy:\", test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwOS7YG7m07S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}