{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoPretrained_EGD_3CNN_FC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w6-6FLzrEEdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep leanring project"
      ]
    },
    {
      "metadata": {
        "id": "SDi7CU_zEKX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Data loading and argumenting"
      ]
    },
    {
      "metadata": {
        "id": "Ci2gPZb6ZIEw",
        "colab_type": "code",
        "outputId": "a52686b7-c3f0-4eb5-b123-9aa2e4e4e4f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "##### before running it, make sure you don't have lots of big files in your google drive\n",
        "##### otherwise it's going to take too long to finish running it before giving the TIMEOUT error\n",
        "##### also save the train_controls, train_patients, val_controls, val_patients to your drive and\n",
        "##### create a \"train\" folder with train_controls, train_patients in it, and \n",
        "##### a \"val\" folder with val_controls, val_patients in it.\n",
        "##### change the train_dir and val_dir in the next cell to the dir of your train and val folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmgcIKHmDCWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Seeds and predefined stuffs"
      ]
    },
    {
      "metadata": {
        "id": "2IFssfAfDLLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(137)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(191)\n",
        "\n",
        "# Dir (Comment out others when you run the code)\n",
        "# e.g. /gdrive/My Drive/deep_learning/new_dataset/test/controls/View2098.jpg\n",
        "#          |                                             |\n",
        "\n",
        "# Kavi's\n",
        "\n",
        "# Daniel's\n",
        "\n",
        "# Chelsea's Probs\n",
        "train_dir = \"/gdrive/My Drive/deep_learning/EGD\"\n",
        "\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "batch_size = 5\n",
        "channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPWlDKH0E5zu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data loading"
      ]
    },
    {
      "metadata": {
        "id": "oeCnbRQ-ZVQO",
        "colab_type": "code",
        "outputId": "fab84849-961e-47f1-8698-116ed79a47e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "# val_datagen = ImageDataGenerator(rescale=1./255)              \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, \n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        shuffle = True,\n",
        "        class_mode='binary')   \n",
        "\n",
        "#Keras takes care of generating labels if the directory structure matches above!\n",
        "label_mapT = train_generator.class_indices\n",
        "print(label_mapT)\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print ('data batch shape:', data_batch.shape)\n",
        "    #print(data_batch)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    #print(labels_batch)\n",
        "    break\n",
        "    \n",
        "nb_train_samples = len(train_generator.filenames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 102 images belonging to 2 classes.\n",
            "{'controls': 0, 'patients': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:2274: DecompressionBombWarning: Image size (171407884 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  DecompressionBombWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data batch shape: (5, 224, 224, 3)\n",
            "labels batch shape: (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "prbgRgBf6Yyr",
        "colab_type": "code",
        "outputId": "36faac63-f793-47b9-d1cf-e4f390db6483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def stacking_samples(dataset_type, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, img_height, img_width, channels))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    i = 0\n",
        "    if dataset_type == \"train\":\n",
        "        for inputs_batch, labels_batch in train_generator:\n",
        "#             features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "            i += 1\n",
        "            print(i * batch_size)\n",
        "            if i * batch_size >= sample_count:\n",
        "                break   \n",
        "#     elif dataset_type == \"valid\":\n",
        "#         for inputs_batch, labels_batch in valid_generator:\n",
        "# #             features_batch = conv_base.predict(inputs_batch)\n",
        "#             features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "#             labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "#             i += 1\n",
        "#             if i * batch_size >= sample_count:\n",
        "#                 break\n",
        "#     else:\n",
        "#         for inputs_batch, labels_batch in test_generator:\n",
        "# #             features_batch = conv_base.predict(inputs_batch)\n",
        "#             features[i * batch_size : (i + 1) * batch_size] = inputs_batch\n",
        "#             labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch)\n",
        "#             i += 1\n",
        "#             if i * batch_size >= sample_count:\n",
        "#                 break\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "train_features, train_labels = stacking_samples(\"train\", nb_train_samples)\n",
        "\n",
        "\n",
        "print(train_features.shape, train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:2274: DecompressionBombWarning: Image size (171407884 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  DecompressionBombWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "50\n",
            "55\n",
            "60\n",
            "65\n",
            "70\n",
            "75\n",
            "80\n",
            "85\n",
            "90\n",
            "95\n",
            "100\n",
            "105\n",
            "(102, 224, 224, 3) (102,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJMLPZnOEWL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Model training"
      ]
    },
    {
      "metadata": {
        "id": "-5V7skIXZQrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Softmax, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Lambda\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "#from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "input_shape = (img_height, img_width, channels)\n",
        "\n",
        "\n",
        "def getModel():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32,(11, 11), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5)) \n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  \n",
        "  opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.1)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SpAGHb4D4nl",
        "colab_type": "code",
        "outputId": "15d1a722-9d31-44ba-df18-62adb2a203df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 214, 214, 32)      11648     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 214, 214, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 107, 107, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 107, 107, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 105, 105, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 105, 105, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 52, 52, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 52, 52, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 50, 50, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 50, 50, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 25, 25, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 25, 25, 32)        128       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               2560128   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 2,590,785\n",
            "Trainable params: 2,590,593\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xo6Iq-inCoqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.1)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TJXQTczdMW_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fhrhARgN8OU6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "metadata": {
        "id": "6Y98Svj39wjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For early stopping\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard, Callback, EarlyStopping\n",
        "\n",
        "class MetricsCheckpoint(Callback):\n",
        "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
        "    def __init__(self, savepath):\n",
        "        super(MetricsCheckpoint, self).__init__()\n",
        "        self.savepath = savepath\n",
        "        self.history = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        np.save(self.savepath, self.history)\n",
        "        \n",
        "callbacks_list = [EarlyStopping(monitor='val_acc', patience=20, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sO2kvmi78eXJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 10-fold"
      ]
    },
    {
      "metadata": {
        "id": "GzGPh1WV8gou",
        "colab_type": "code",
        "outputId": "ea95efc2-27e9-4480-e9c5-b0a0471aa7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7155
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "\n",
        "features = train_features\n",
        "labels = train_labels\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "accs = []\n",
        "for train_index, val_index in kf.split(features):\n",
        "  x_train = features[train_index]\n",
        "  y_train = labels[train_index]\n",
        "  x_val = features[val_index]\n",
        "  y_val = labels[val_index]\n",
        "  \n",
        "  model = getModel()\n",
        "\n",
        "  history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))\n",
        "  accs.append(history.history['val_acc'][-1])\n",
        "\n",
        "print(accs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 91 samples, validate on 11 samples\n",
            "Epoch 1/20\n",
            "91/91 [==============================] - 9s 104ms/step - loss: 2.4585 - acc: 0.6484 - val_loss: 3.0460 - val_acc: 0.7273\n",
            "Epoch 2/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 1.2230 - acc: 0.7692 - val_loss: 2.8968 - val_acc: 0.6364\n",
            "Epoch 3/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.4275 - acc: 0.8681 - val_loss: 2.5990 - val_acc: 0.6364\n",
            "Epoch 4/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.5070 - acc: 0.8681 - val_loss: 2.8275 - val_acc: 0.7273\n",
            "Epoch 5/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.4353 - acc: 0.8901 - val_loss: 3.2180 - val_acc: 0.5455\n",
            "Epoch 6/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.1251 - acc: 0.9670 - val_loss: 3.1026 - val_acc: 0.5455\n",
            "Epoch 7/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.1609 - acc: 0.9560 - val_loss: 3.4208 - val_acc: 0.5455\n",
            "Epoch 8/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0730 - acc: 0.9780 - val_loss: 3.8127 - val_acc: 0.5455\n",
            "Epoch 9/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0712 - acc: 0.9890 - val_loss: 4.5241 - val_acc: 0.4545\n",
            "Epoch 10/20\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.0605 - acc: 0.9780 - val_loss: 4.1959 - val_acc: 0.4545\n",
            "Epoch 11/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.1151 - acc: 0.9560 - val_loss: 3.9904 - val_acc: 0.5455\n",
            "Epoch 12/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0358 - acc: 0.9890 - val_loss: 3.9883 - val_acc: 0.5455\n",
            "Epoch 13/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0410 - acc: 0.9890 - val_loss: 4.0880 - val_acc: 0.5455\n",
            "Epoch 14/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0486 - acc: 0.9670 - val_loss: 4.0019 - val_acc: 0.5455\n",
            "Epoch 15/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0591 - acc: 0.9560 - val_loss: 3.8647 - val_acc: 0.5455\n",
            "Epoch 16/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0939 - acc: 0.9560 - val_loss: 3.8640 - val_acc: 0.5455\n",
            "Epoch 17/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0712 - acc: 0.9670 - val_loss: 3.9054 - val_acc: 0.5455\n",
            "Epoch 18/20\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.0517 - acc: 0.9780 - val_loss: 3.8851 - val_acc: 0.5455\n",
            "Epoch 19/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0133 - acc: 1.0000 - val_loss: 3.8859 - val_acc: 0.5455\n",
            "Epoch 20/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.0489 - acc: 0.9780 - val_loss: 3.3450 - val_acc: 0.5455\n",
            "Train on 91 samples, validate on 11 samples\n",
            "Epoch 1/20\n",
            "91/91 [==============================] - 10s 110ms/step - loss: 3.2521 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 2/20\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 3.5424 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 3/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 3.5424 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 4/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 3.5424 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 5/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 3.5424 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 6/20\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 3.4976 - acc: 0.7802 - val_loss: 2.9306 - val_acc: 0.8182\n",
            "Epoch 7/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 3.5394 - acc: 0.7802 - val_loss: 3.0220 - val_acc: 0.7273\n",
            "Epoch 8/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 3.4146 - acc: 0.7802 - val_loss: 3.5812 - val_acc: 0.5455\n",
            "Epoch 9/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 2.7753 - acc: 0.8022 - val_loss: 6.2294 - val_acc: 0.4545\n",
            "Epoch 10/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 2.5559 - acc: 0.8022 - val_loss: 9.1680 - val_acc: 0.2727\n",
            "Epoch 11/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 2.2089 - acc: 0.7473 - val_loss: 8.3415 - val_acc: 0.2727\n",
            "Epoch 12/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 1.6395 - acc: 0.8242 - val_loss: 8.6024 - val_acc: 0.2727\n",
            "Epoch 13/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 1.7638 - acc: 0.7143 - val_loss: 6.3887 - val_acc: 0.2727\n",
            "Epoch 14/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 1.2908 - acc: 0.7363 - val_loss: 2.7752 - val_acc: 0.5455\n",
            "Epoch 15/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 1.0686 - acc: 0.8132 - val_loss: 1.8403 - val_acc: 0.5455\n",
            "Epoch 16/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.9874 - acc: 0.8462 - val_loss: 1.8357 - val_acc: 0.5455\n",
            "Epoch 17/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.9227 - acc: 0.8791 - val_loss: 1.6933 - val_acc: 0.7273\n",
            "Epoch 18/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.8499 - acc: 0.8462 - val_loss: 1.5898 - val_acc: 0.6364\n",
            "Epoch 19/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.8398 - acc: 0.8462 - val_loss: 1.5240 - val_acc: 0.6364\n",
            "Epoch 20/20\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.6509 - acc: 0.9121 - val_loss: 1.5715 - val_acc: 0.8182\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 9s 103ms/step - loss: 3.4039 - acc: 0.7717 - val_loss: 1.6118 - val_acc: 0.9000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.4270 - acc: 0.7500 - val_loss: 1.6124 - val_acc: 0.9000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.4569 - acc: 0.7065 - val_loss: 1.3834 - val_acc: 0.8000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.9373 - acc: 0.7174 - val_loss: 1.6056 - val_acc: 0.8000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.4301 - acc: 0.8478 - val_loss: 0.9130 - val_acc: 0.8000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.2587 - acc: 0.9348 - val_loss: 1.0594 - val_acc: 0.8000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.2355 - acc: 0.8913 - val_loss: 1.3390 - val_acc: 0.9000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1725 - acc: 0.9130 - val_loss: 1.6129 - val_acc: 0.9000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0999 - acc: 0.9565 - val_loss: 1.6131 - val_acc: 0.9000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0965 - acc: 0.9565 - val_loss: 1.3916 - val_acc: 0.9000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0413 - acc: 0.9783 - val_loss: 1.1756 - val_acc: 0.9000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0586 - acc: 0.9783 - val_loss: 1.0525 - val_acc: 0.9000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0329 - acc: 0.9891 - val_loss: 1.1448 - val_acc: 0.9000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0802 - acc: 0.9783 - val_loss: 0.9678 - val_acc: 0.9000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0517 - acc: 0.9783 - val_loss: 1.2432 - val_acc: 0.9000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 1.3461 - val_acc: 0.9000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0747 - acc: 0.9674 - val_loss: 1.1723 - val_acc: 0.9000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 1.1348 - val_acc: 0.9000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 1.2644 - val_acc: 0.9000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0301 - acc: 1.0000 - val_loss: 1.2199 - val_acc: 0.9000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 10s 108ms/step - loss: 3.6035 - acc: 0.7500 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 10s 105ms/step - loss: 3.2532 - acc: 0.7717 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.2315 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.1535 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.1536 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9945 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9774 - acc: 0.8152 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.1115 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.0230 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9787 - acc: 0.8152 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9227 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9313 - acc: 0.8152 - val_loss: 4.2970 - val_acc: 0.7000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.8986 - acc: 0.8152 - val_loss: 3.9992 - val_acc: 0.7000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.8923 - acc: 0.8152 - val_loss: 3.6540 - val_acc: 0.7000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.2359 - acc: 0.7935 - val_loss: 3.3795 - val_acc: 0.7000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.7641 - acc: 0.8152 - val_loss: 3.4117 - val_acc: 0.7000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.7712 - acc: 0.8152 - val_loss: 3.6067 - val_acc: 0.7000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.9793 - acc: 0.8152 - val_loss: 3.6584 - val_acc: 0.7000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.8051 - acc: 0.8261 - val_loss: 3.4491 - val_acc: 0.7000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 10s 108ms/step - loss: 4.5898 - acc: 0.5870 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 2.4390 - acc: 0.6522 - val_loss: 1.2390 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.0255 - acc: 0.7609 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.5080 - acc: 0.8261 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4042 - acc: 0.8696 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3772 - acc: 0.8478 - val_loss: 0.0053 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2097 - acc: 0.9457 - val_loss: 0.0146 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2273 - acc: 0.9130 - val_loss: 0.0159 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1547 - acc: 0.9565 - val_loss: 0.0189 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0788 - acc: 0.9783 - val_loss: 0.0155 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1510 - acc: 0.9457 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0689 - acc: 0.9783 - val_loss: 0.2874 - val_acc: 0.9000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0832 - acc: 0.9565 - val_loss: 0.1424 - val_acc: 0.9000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1372 - acc: 0.9348 - val_loss: 0.1037 - val_acc: 0.9000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0641 - acc: 0.9674 - val_loss: 0.1612 - val_acc: 0.9000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0614 - acc: 0.9674 - val_loss: 0.1078 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0662 - acc: 0.9674 - val_loss: 0.0857 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0329 - acc: 0.9891 - val_loss: 0.1936 - val_acc: 0.9000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0369 - acc: 0.9891 - val_loss: 0.0538 - val_acc: 1.0000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 9s 103ms/step - loss: 3.3045 - acc: 0.7283 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5017 - acc: 0.7717 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.2360 - acc: 0.7826 - val_loss: 3.2237 - val_acc: 0.8000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 2.0412 - acc: 0.8261 - val_loss: 2.6541 - val_acc: 0.8000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.5274 - acc: 0.6957 - val_loss: 1.6749 - val_acc: 0.8000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4820 - acc: 0.8478 - val_loss: 1.3067 - val_acc: 0.8000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.4166 - acc: 0.7935 - val_loss: 1.1267 - val_acc: 0.8000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4327 - acc: 0.8370 - val_loss: 1.0973 - val_acc: 0.8000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2284 - acc: 0.8804 - val_loss: 1.2623 - val_acc: 0.8000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2020 - acc: 0.9348 - val_loss: 2.0698 - val_acc: 0.8000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1841 - acc: 0.9457 - val_loss: 1.6525 - val_acc: 0.7000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2021 - acc: 0.9130 - val_loss: 2.5265 - val_acc: 0.7000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1074 - acc: 0.9565 - val_loss: 2.5283 - val_acc: 0.7000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1421 - acc: 0.9565 - val_loss: 2.0510 - val_acc: 0.7000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0964 - acc: 0.9674 - val_loss: 1.9178 - val_acc: 0.7000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0640 - acc: 0.9783 - val_loss: 2.5393 - val_acc: 0.7000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0675 - acc: 0.9891 - val_loss: 1.8869 - val_acc: 0.7000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1031 - acc: 0.9565 - val_loss: 1.9547 - val_acc: 0.7000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0537 - acc: 0.9783 - val_loss: 1.7124 - val_acc: 0.7000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0686 - acc: 0.9783 - val_loss: 1.8797 - val_acc: 0.7000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 9s 100ms/step - loss: 3.1687 - acc: 0.8043 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3288 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.3287 - acc: 0.7935 - val_loss: 4.8354 - val_acc: 0.7000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 9s 100ms/step - loss: 2.8932 - acc: 0.7717 - val_loss: 6.4472 - val_acc: 0.6000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 2.7907 - acc: 0.7283 - val_loss: 5.4734 - val_acc: 0.6000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.8987 - acc: 0.7500 - val_loss: 3.3139 - val_acc: 0.4000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.1672 - acc: 0.8043 - val_loss: 3.4757 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.5942 - acc: 0.8913 - val_loss: 5.2827 - val_acc: 0.6000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4618 - acc: 0.9022 - val_loss: 4.2907 - val_acc: 0.6000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4506 - acc: 0.8804 - val_loss: 2.1198 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3763 - acc: 0.9239 - val_loss: 2.3299 - val_acc: 0.6000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1127 - acc: 0.9674 - val_loss: 2.0386 - val_acc: 0.6000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2166 - acc: 0.9130 - val_loss: 1.8223 - val_acc: 0.4000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0744 - acc: 0.9565 - val_loss: 1.7887 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0823 - acc: 0.9565 - val_loss: 2.0994 - val_acc: 0.4000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0516 - acc: 0.9783 - val_loss: 2.4820 - val_acc: 0.6000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0516 - acc: 0.9891 - val_loss: 2.1361 - val_acc: 0.4000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0260 - acc: 0.9891 - val_loss: 1.9789 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0472 - acc: 0.9674 - val_loss: 2.3191 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0383 - acc: 0.9891 - val_loss: 2.1841 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0284 - acc: 0.9891 - val_loss: 2.2134 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0473 - acc: 0.9891 - val_loss: 2.3567 - val_acc: 0.4000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0265 - acc: 0.9891 - val_loss: 2.3157 - val_acc: 0.4000\n",
            "Train on 92 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "92/92 [==============================] - 10s 107ms/step - loss: 3.5502 - acc: 0.7609 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 2/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 3/20\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 4/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 5/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 6/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 7/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 8/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 9/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 10/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 11/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 12/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 13/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 14/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 15/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 16/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 17/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 18/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 19/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "Epoch 20/20\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 3.5039 - acc: 0.7826 - val_loss: 3.2236 - val_acc: 0.8000\n",
            "[0.5454545590010557, 0.8181818290190264, 0.9000000059604645, 0.800000011920929, 0.7000000178813934, 1.0, 0.7000000029802322, 0.7000000178813934, 0.4000000134110451, 0.800000011920929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ofamskm9QwQG",
        "colab_type": "code",
        "outputId": "b755375b-d1e7-481f-ea55-c90eb5337fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ave = np.mean(accs)\n",
        "print(\"final accuracy\", ave)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final accuracy 0.7363636469976468\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}