{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoPretrained_Kaggle_3CNN_FC",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w6-6FLzrEEdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep leanring project"
      ]
    },
    {
      "metadata": {
        "id": "SDi7CU_zEKX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Data loading and argumenting"
      ]
    },
    {
      "metadata": {
        "id": "Ci2gPZb6ZIEw",
        "colab_type": "code",
        "outputId": "c2217d64-5524-4b21-8e98-a682ffc6f029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "##### before running it, make sure you don't have lots of big files in your google drive\n",
        "##### otherwise it's going to take too long to finish running it before giving the TIMEOUT error\n",
        "##### also save the train_controls, train_patients, val_controls, val_patients to your drive and\n",
        "##### create a \"train\" folder with train_controls, train_patients in it, and \n",
        "##### a \"val\" folder with val_controls, val_patients in it.\n",
        "##### change the train_dir and val_dir in the next cell to the dir of your train and val folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmgcIKHmDCWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Seeds and predefined stuffs"
      ]
    },
    {
      "metadata": {
        "id": "2IFssfAfDLLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(137)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(191)\n",
        "\n",
        "# Dir (Comment out others when you run the code)\n",
        "# e.g. /gdrive/My Drive/deep_learning/new_dataset/test/controls/View2098.jpg\n",
        "#          |                                             |\n",
        "\n",
        "# Kavi's\n",
        "\n",
        "# Daniel's\n",
        "\n",
        "# Chelsea's Probs\n",
        "train_dir = \"/gdrive/My Drive/kaggle_dataset/train\"\n",
        "val_dir = \"/gdrive/My Drive/kaggle_dataset/val\"\n",
        "test_dir = \"/gdrive/My Drive/kaggle_dataset/test\"\n",
        "\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "batch_size = 200\n",
        "channels = 3\n",
        "epochs = 50\n",
        "nb_train_samples = 8000\n",
        "nb_valid_samples = 32\n",
        "nb_test_samples = 968\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPWlDKH0E5zu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data loading"
      ]
    },
    {
      "metadata": {
        "id": "oeCnbRQ-ZVQO",
        "colab_type": "code",
        "outputId": "c9fcf999-8e42-4d0a-ec78-998d86f4cc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "val_datagen = ImageDataGenerator(rescale=1./255)              \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, \n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        shuffle = True,\n",
        "        class_mode='categorical')   \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle = True,\n",
        "    class_mode='categorical') #weight toward one class or another\n",
        "\n",
        "#Keras takes care of generating labels if the directory structure matches above!\n",
        "label_mapT = train_generator.class_indices\n",
        "print(label_mapT)\n",
        "\n",
        "label_mapV = validation_generator.class_indices\n",
        "print(label_mapV)\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print ('data batch shape:', data_batch.shape)\n",
        "    #print(data_batch)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    #print(labels_batch)\n",
        "    break\n",
        "    \n",
        "nb_train_samples = len(train_generator.filenames)\n",
        "nb_validation_samples = len(validation_generator.filenames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n",
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "data batch shape: (200, 224, 224, 3)\n",
            "labels batch shape: (200, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJMLPZnOEWL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Model training"
      ]
    },
    {
      "metadata": {
        "id": "-5V7skIXZQrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Softmax, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Lambda\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "#from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "input_shape = (img_height, img_width, channels)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) \n",
        "\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SpAGHb4D4nl",
        "colab_type": "code",
        "outputId": "35a530ba-60aa-4005-cd2b-f4488abcb1c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1042
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 222, 222, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 111, 111, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 109, 109, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 54, 54, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 52, 52, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 26, 26, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 10, 10, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 5, 5, 64)          256       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               409856    \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4)                 1028      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 514,404\n",
            "Trainable params: 513,892\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Y98Svj39wjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For early stopping\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard, Callback, EarlyStopping\n",
        "\n",
        "class MetricsCheckpoint(Callback):\n",
        "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
        "    def __init__(self, savepath):\n",
        "        super(MetricsCheckpoint, self).__init__()\n",
        "        self.savepath = savepath\n",
        "        self.history = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        np.save(self.savepath, self.history)\n",
        "        \n",
        "callbacks_list = [EarlyStopping(monitor='val_acc', patience=10, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jUYHYFFbeUd",
        "colab_type": "code",
        "outputId": "2e16deb3-5421-4773-9567-176f2144eec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.1)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "val_step = 1 ## nb_validation_samples // batch_size\n",
        "\n",
        "    \n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_step,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks_list+[MetricsCheckpoint('logs')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "40/40 [==============================] - 50s 1s/step - loss: 1.2900 - acc: 0.4076 - val_loss: 1.0041 - val_acc: 0.6562\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.9343 - acc: 0.6179 - val_loss: 0.7630 - val_acc: 0.6875\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 48s 1s/step - loss: 0.7580 - acc: 0.6969 - val_loss: 0.6197 - val_acc: 0.7812\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.6757 - acc: 0.7332 - val_loss: 0.5495 - val_acc: 0.7500\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.6209 - acc: 0.7550 - val_loss: 0.5450 - val_acc: 0.7500\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.5724 - acc: 0.7761 - val_loss: 0.5466 - val_acc: 0.7812\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.5387 - acc: 0.7876 - val_loss: 0.5354 - val_acc: 0.8125\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.5180 - acc: 0.8014 - val_loss: 0.4961 - val_acc: 0.8125\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 48s 1s/step - loss: 0.4958 - acc: 0.8124 - val_loss: 0.4759 - val_acc: 0.8125\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4825 - acc: 0.8129 - val_loss: 0.4640 - val_acc: 0.8125\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4627 - acc: 0.8258 - val_loss: 0.4551 - val_acc: 0.8125\n",
            "Epoch 12/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4504 - acc: 0.8291 - val_loss: 0.4457 - val_acc: 0.8125\n",
            "Epoch 13/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4448 - acc: 0.8259 - val_loss: 0.4591 - val_acc: 0.8438\n",
            "Epoch 14/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4265 - acc: 0.8382 - val_loss: 0.4346 - val_acc: 0.8438\n",
            "Epoch 15/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4135 - acc: 0.8436 - val_loss: 0.4226 - val_acc: 0.8438\n",
            "Epoch 16/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4136 - acc: 0.8466 - val_loss: 0.4133 - val_acc: 0.8438\n",
            "Epoch 17/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.4048 - acc: 0.8483 - val_loss: 0.3879 - val_acc: 0.8438\n",
            "Epoch 18/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3921 - acc: 0.8524 - val_loss: 0.4169 - val_acc: 0.8438\n",
            "Epoch 19/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3913 - acc: 0.8518 - val_loss: 0.4033 - val_acc: 0.8438\n",
            "Epoch 20/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3872 - acc: 0.8566 - val_loss: 0.4420 - val_acc: 0.8438\n",
            "Epoch 21/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3785 - acc: 0.8596 - val_loss: 0.3889 - val_acc: 0.8438\n",
            "Epoch 22/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3763 - acc: 0.8580 - val_loss: 0.3999 - val_acc: 0.8438\n",
            "Epoch 23/50\n",
            "40/40 [==============================] - 47s 1s/step - loss: 0.3646 - acc: 0.8611 - val_loss: 0.3912 - val_acc: 0.8438\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ca69d97b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "_D5pOC4QGAVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Prediction"
      ]
    },
    {
      "metadata": {
        "id": "aTHb58g2GHA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode=\"rgb\",\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "filenames = test_generator.filenames\n",
        "nb_samples = len(filenames)\n",
        "\n",
        "predict = model.predict_generator(test_generator, steps = np.ceil(nb_samples / batch_size))\n",
        "    \n",
        "# Getting categorical prediction\n",
        "predict = np.round_(predict)\n",
        "\n",
        "model.evaluate_generator(test_generator, steps = np.ceil(nb_samples / batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lW9MTZhit2Xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = test_generator.classes\n",
        "predictions = predict\n",
        "\n",
        "print(labels.shape)\n",
        "print(predictions.shape)\n",
        "\n",
        "predictions = predictions.reshape(len(labels))\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "FP_list = []\n",
        "FN_list = []\n",
        "\n",
        "#FP\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 0 and predictions[i] == 1:\n",
        "    FP_list.append(filenames[i])\n",
        "\n",
        "#FN\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 1 and predictions[i] == 0:\n",
        "    FN_list.append(filenames[i])\n",
        "\n",
        "print(len(FP_list))\n",
        "print(len(FN_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-6tHxahEdjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 Model visualization"
      ]
    },
    {
      "metadata": {
        "id": "CF1NjQGqEw99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Preamble downlaoding"
      ]
    },
    {
      "metadata": {
        "id": "RsgxOgWfEjU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/raghakot/keras-vis.git\n",
        "\n",
        "from keras.applications import ResNet50\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "# Hide warnings on Jupyter Notebook\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rg4_s2g1E3C0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Display images (to make sure the intended ones are shown)"
      ]
    },
    {
      "metadata": {
        "id": "B7lDTXIlE_qm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "\n",
        "\n",
        "img1 = utils.load_img(test_dir + '/controls/View2098.png', target_size=(img_height, img_width))\n",
        "img2 = utils.load_img(test_dir + '/patients/19105.png', target_size=(img_height, img_width))\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img1)\n",
        "ax[1].imshow(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbyZkuuDC4B3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3 Attention heatmap displayed on the gray-scale image"
      ]
    },
    {
      "metadata": {
        "id": "omrSszuCe9vU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 Showing and saving FP"
      ]
    },
    {
      "metadata": {
        "id": "A7lFrsNvdrd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "   #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "FPFN_dir =  \"/gdrive/My Drive/deep_learning/FP&FN\"\n",
        "\n",
        "import os\n",
        "FN_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FN\")\n",
        "FP_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FP\")\n",
        "\n",
        "FP_imglist = []\n",
        "for i in range(len(FP_list)):\n",
        "   FP_imglist.append(utils.load_img(FPFN_dir + '/FP/' + FP_list[i], target_size=(img_height, img_width)))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "for modifier in [None]:\n",
        "   plt.figure()\n",
        "   f, ax = plt.subplots(1, 2)\n",
        "   plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "   for i, img in enumerate(FP_imglist):\n",
        "#         grads = visualize_saliency(model, layer_idx, filter_indices=20, seed_input=img)\n",
        "\n",
        "#         # visualize grads as heatmap\n",
        "#         ax[i].imshow(grads, cmap='jet')\n",
        "\n",
        "       grads = visualize_cam(model, layer_idx, filter_indices=0,\n",
        "                             seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                             backprop_modifier=modifier)\n",
        "       jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "       grey_img = converter(img)\n",
        "\n",
        "       save_img = overlay(jet_heatmap, grey_img)\n",
        "       im = Image.fromarray(save_img)\n",
        "       im.save(str(i)+\".png\")\n",
        "       files.download(str(i)+\".png\")\n",
        "\n",
        "#         ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hP3ZIg7fAvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 FN"
      ]
    },
    {
      "metadata": {
        "id": "zpQ_xfzYfCZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "   #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "FPFN_dir =  \"/gdrive/My Drive/deep_learning/FP&FN\"\n",
        "\n",
        "import os\n",
        "FN_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FN\")\n",
        "FP_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FP\")\n",
        "\n",
        "FN_imglist = []\n",
        "for i in range(len(FN_list)):\n",
        "   FN_imglist.append(utils.load_img(FPFN_dir + '/FN/' + FN_list[i], target_size=(img_height, img_width)))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "for modifier in [None]:\n",
        "   plt.figure()\n",
        "   f, ax = plt.subplots(1, 2)\n",
        "   plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "   for i, img in enumerate(FN_imglist):\n",
        "\n",
        "       grads = visualize_cam(model, layer_idx, filter_indices=0,\n",
        "                             seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                             backprop_modifier=modifier)\n",
        "       jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "       grey_img = converter(img)\n",
        "\n",
        "       save_img = overlay(jet_heatmap, grey_img)\n",
        "       im = Image.fromarray(save_img)\n",
        "       im.save(str(i)+\".png\")\n",
        "       files.download(str(i)+\".png\")\n",
        "\n",
        "#         ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSBG6ghXI12K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "    #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "for modifier in [None, 'guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 2)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        \n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=0, \n",
        "                              seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "        grey_img = converter(img)\n",
        "\n",
        "        ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi9fdsBwIoVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3 Attention heatmap displayed on the origin image"
      ]
    },
    {
      "metadata": {
        "id": "s_r5wBNZ57Wt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for modifier in [None, 'guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 4)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=0, \n",
        "                              seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "        ax[i].imshow(overlay(jet_heatmap, img))\n",
        "        ax[i + 2].imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}