{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoPretrained_Kaggle_3CNN_FC",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w6-6FLzrEEdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep leanring project"
      ]
    },
    {
      "metadata": {
        "id": "SDi7CU_zEKX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Data loading and argumenting"
      ]
    },
    {
      "metadata": {
        "id": "Ci2gPZb6ZIEw",
        "colab_type": "code",
        "outputId": "af34c252-385a-4635-9efd-b3fedb34e5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "##### before running it, make sure you don't have lots of big files in your google drive\n",
        "##### otherwise it's going to take too long to finish running it before giving the TIMEOUT error\n",
        "##### also save the train_controls, train_patients, val_controls, val_patients to your drive and\n",
        "##### create a \"train\" folder with train_controls, train_patients in it, and \n",
        "##### a \"val\" folder with val_controls, val_patients in it.\n",
        "##### change the train_dir and val_dir in the next cell to the dir of your train and val folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmgcIKHmDCWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Seeds and predefined stuffs"
      ]
    },
    {
      "metadata": {
        "id": "2IFssfAfDLLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(137)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(191)\n",
        "\n",
        "# Dir (Comment out others when you run the code)\n",
        "# e.g. /gdrive/My Drive/deep_learning/new_dataset/test/controls/View2098.jpg\n",
        "#          |                                             |\n",
        "\n",
        "# Kavi's\n",
        "\n",
        "# Daniel's\n",
        "\n",
        "# Chelsea's Probs\n",
        "train_dir = \"/gdrive/My Drive/kaggle_dataset/train\"\n",
        "val_dir = \"/gdrive/My Drive/kaggle_dataset/val\"\n",
        "test_dir = \"/gdrive/My Drive/kaggle_dataset/test\"\n",
        "\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "batch_size = 200\n",
        "channels = 3\n",
        "epochs = 50\n",
        "nb_train_samples = 8000\n",
        "nb_valid_samples = 32\n",
        "nb_test_samples = 968\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPWlDKH0E5zu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data loading"
      ]
    },
    {
      "metadata": {
        "id": "oeCnbRQ-ZVQO",
        "colab_type": "code",
        "outputId": "41403386-b9e3-493a-d87d-197c3f85dbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "val_datagen = ImageDataGenerator(rescale=1./255)              \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, \n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        shuffle = True,\n",
        "        class_mode='categorical')   \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle = True,\n",
        "    class_mode='categorical') #weight toward one class or another\n",
        "\n",
        "#Keras takes care of generating labels if the directory structure matches above!\n",
        "label_mapT = train_generator.class_indices\n",
        "print(label_mapT)\n",
        "\n",
        "label_mapV = validation_generator.class_indices\n",
        "print(label_mapV)\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print ('data batch shape:', data_batch.shape)\n",
        "    #print(data_batch)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    #print(labels_batch)\n",
        "    break\n",
        "    \n",
        "nb_train_samples = len(train_generator.filenames)\n",
        "nb_validation_samples = len(validation_generator.filenames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n",
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}\n",
            "data batch shape: (200, 224, 224, 3)\n",
            "labels batch shape: (200, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJMLPZnOEWL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Model training"
      ]
    },
    {
      "metadata": {
        "id": "-5V7skIXZQrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Softmax, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Lambda\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "#from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "input_shape = (img_height, img_width, channels)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(11, 11), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32,(3, 3), input_shape=input_shape))#, kernel_regularizer=regularizers.l1(0.01))) \n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) \n",
        "\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SpAGHb4D4nl",
        "colab_type": "code",
        "outputId": "a8c7389c-65c9-44a4-e4bd-7149af57c2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 214, 214, 32)      11648     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 214, 214, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 107, 107, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 107, 107, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 105, 105, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 105, 105, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 52, 52, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 52, 52, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 50, 50, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 50, 50, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 25, 25, 32)        128       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               2560128   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 516       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 2,591,172\n",
            "Trainable params: 2,590,980\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Y98Svj39wjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For early stopping\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard, Callback, EarlyStopping\n",
        "\n",
        "class MetricsCheckpoint(Callback):\n",
        "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
        "    def __init__(self, savepath):\n",
        "        super(MetricsCheckpoint, self).__init__()\n",
        "        self.savepath = savepath\n",
        "        self.history = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        np.save(self.savepath, self.history)\n",
        "        \n",
        "callbacks_list = [EarlyStopping(monitor='val_acc', patience=10, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jUYHYFFbeUd",
        "colab_type": "code",
        "outputId": "2c21a9fd-dbbf-457f-b92e-7eae0076968c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.1)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "val_step = 1 ## nb_validation_samples // batch_size\n",
        "\n",
        "    \n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_step,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks_list+[MetricsCheckpoint('logs')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "40/40 [==============================] - 4409s 110s/step - loss: 1.4384 - acc: 0.2672 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 50s 1s/step - loss: 1.3865 - acc: 0.2720 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 50s 1s/step - loss: 1.3831 - acc: 0.2801 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 50s 1s/step - loss: 1.3773 - acc: 0.2818 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 50s 1s/step - loss: 1.3760 - acc: 0.2930 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.3600 - acc: 0.3075 - val_loss: 1.3863 - val_acc: 0.2500\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3608 - acc: 0.3094 - val_loss: 1.3861 - val_acc: 0.2812\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3569 - acc: 0.3150 - val_loss: 1.3566 - val_acc: 0.4062\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3459 - acc: 0.3175 - val_loss: 1.3508 - val_acc: 0.4375\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3292 - acc: 0.3225 - val_loss: 1.3687 - val_acc: 0.5000\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3294 - acc: 0.3196 - val_loss: 1.3233 - val_acc: 0.4062\n",
            "Epoch 12/50\n",
            "40/40 [==============================] - 52s 1s/step - loss: 1.3199 - acc: 0.3280 - val_loss: 1.3336 - val_acc: 0.5000\n",
            "Epoch 13/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.3071 - acc: 0.3280 - val_loss: 1.3541 - val_acc: 0.5938\n",
            "Epoch 14/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2986 - acc: 0.3394 - val_loss: 1.3310 - val_acc: 0.5312\n",
            "Epoch 15/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2915 - acc: 0.3438 - val_loss: 1.3306 - val_acc: 0.5000\n",
            "Epoch 16/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2848 - acc: 0.3486 - val_loss: 1.2954 - val_acc: 0.4375\n",
            "Epoch 17/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2765 - acc: 0.3468 - val_loss: 1.2920 - val_acc: 0.4375\n",
            "Epoch 18/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2700 - acc: 0.3429 - val_loss: 1.2643 - val_acc: 0.4375\n",
            "Epoch 19/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2602 - acc: 0.3536 - val_loss: 1.2908 - val_acc: 0.4062\n",
            "Epoch 20/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2510 - acc: 0.3580 - val_loss: 1.2599 - val_acc: 0.3750\n",
            "Epoch 21/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2383 - acc: 0.3521 - val_loss: 1.2362 - val_acc: 0.4062\n",
            "Epoch 22/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2316 - acc: 0.3529 - val_loss: 1.2714 - val_acc: 0.4375\n",
            "Epoch 23/50\n",
            "40/40 [==============================] - 51s 1s/step - loss: 1.2207 - acc: 0.3590 - val_loss: 1.2486 - val_acc: 0.4062\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa16fa2ae48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "_D5pOC4QGAVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Prediction"
      ]
    },
    {
      "metadata": {
        "id": "aTHb58g2GHA8",
        "colab_type": "code",
        "outputId": "e072106a-9bbd-47fc-eef7-c97eb9e26874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        color_mode=\"rgb\",\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "filenames = test_generator.filenames\n",
        "nb_samples = len(filenames)\n",
        "\n",
        "predict = model.predict_generator(test_generator, steps = np.ceil(nb_samples / batch_size))\n",
        "    \n",
        "# Getting categorical prediction\n",
        "predict = np.round_(predict)\n",
        "\n",
        "model.evaluate_generator(test_generator, steps = np.ceil(nb_samples / batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 968 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2281682565192547, 0.4359504077429614]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "lW9MTZhit2Xl",
        "colab_type": "code",
        "outputId": "38543241-de06-49ba-cadf-6fbc420b16d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "cell_type": "code",
      "source": [
        "labels = test_generator.classes\n",
        "predictions = predict\n",
        "\n",
        "print(labels.shape)\n",
        "print(predictions.shape)\n",
        "\n",
        "predictions = predictions.reshape(len(labels))\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "FP_list = []\n",
        "FN_list = []\n",
        "\n",
        "#FP\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 0 and predictions[i] == 1:\n",
        "    FP_list.append(filenames[i])\n",
        "\n",
        "#FN\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 1 and predictions[i] == 0:\n",
        "    FN_list.append(filenames[i])\n",
        "\n",
        "print(len(FP_list))\n",
        "print(len(FN_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(968,)\n",
            "(968, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6bac8ab5b62e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3872 into shape (968,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "s-6tHxahEdjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 Model visualization"
      ]
    },
    {
      "metadata": {
        "id": "CF1NjQGqEw99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Preamble downlaoding"
      ]
    },
    {
      "metadata": {
        "id": "RsgxOgWfEjU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/raghakot/keras-vis.git\n",
        "\n",
        "from keras.applications import ResNet50\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "# Hide warnings on Jupyter Notebook\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rg4_s2g1E3C0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Display images (to make sure the intended ones are shown)"
      ]
    },
    {
      "metadata": {
        "id": "B7lDTXIlE_qm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "\n",
        "\n",
        "img1 = utils.load_img(test_dir + '/controls/View2098.png', target_size=(img_height, img_width))\n",
        "img2 = utils.load_img(test_dir + '/patients/19105.png', target_size=(img_height, img_width))\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img1)\n",
        "ax[1].imshow(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbyZkuuDC4B3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3 Attention heatmap displayed on the gray-scale image"
      ]
    },
    {
      "metadata": {
        "id": "omrSszuCe9vU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 Showing and saving FP"
      ]
    },
    {
      "metadata": {
        "id": "A7lFrsNvdrd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "   #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "FPFN_dir =  \"/gdrive/My Drive/deep_learning/FP&FN\"\n",
        "\n",
        "import os\n",
        "FN_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FN\")\n",
        "FP_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FP\")\n",
        "\n",
        "FP_imglist = []\n",
        "for i in range(len(FP_list)):\n",
        "   FP_imglist.append(utils.load_img(FPFN_dir + '/FP/' + FP_list[i], target_size=(img_height, img_width)))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "for modifier in [None]:\n",
        "   plt.figure()\n",
        "   f, ax = plt.subplots(1, 2)\n",
        "   plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "   for i, img in enumerate(FP_imglist):\n",
        "#         grads = visualize_saliency(model, layer_idx, filter_indices=20, seed_input=img)\n",
        "\n",
        "#         # visualize grads as heatmap\n",
        "#         ax[i].imshow(grads, cmap='jet')\n",
        "\n",
        "       grads = visualize_cam(model, layer_idx, filter_indices=0,\n",
        "                             seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                             backprop_modifier=modifier)\n",
        "       jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "       grey_img = converter(img)\n",
        "\n",
        "       save_img = overlay(jet_heatmap, grey_img)\n",
        "       im = Image.fromarray(save_img)\n",
        "       im.save(str(i)+\".png\")\n",
        "       files.download(str(i)+\".png\")\n",
        "\n",
        "#         ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hP3ZIg7fAvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 FN"
      ]
    },
    {
      "metadata": {
        "id": "zpQ_xfzYfCZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "   #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "FPFN_dir =  \"/gdrive/My Drive/deep_learning/FP&FN\"\n",
        "\n",
        "import os\n",
        "FN_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FN\")\n",
        "FP_list = os.listdir(\"/gdrive/My Drive/deep_learning/FP&FN/FP\")\n",
        "\n",
        "FN_imglist = []\n",
        "for i in range(len(FN_list)):\n",
        "   FN_imglist.append(utils.load_img(FPFN_dir + '/FN/' + FN_list[i], target_size=(img_height, img_width)))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "for modifier in [None]:\n",
        "   plt.figure()\n",
        "   f, ax = plt.subplots(1, 2)\n",
        "   plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "   for i, img in enumerate(FN_imglist):\n",
        "\n",
        "       grads = visualize_cam(model, layer_idx, filter_indices=0,\n",
        "                             seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                             backprop_modifier=modifier)\n",
        "       jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "       grey_img = converter(img)\n",
        "\n",
        "       save_img = overlay(jet_heatmap, grey_img)\n",
        "       im = Image.fromarray(save_img)\n",
        "       im.save(str(i)+\".png\")\n",
        "       files.download(str(i)+\".png\")\n",
        "\n",
        "#         ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSBG6ghXI12K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "from vis.visualization import visualize_cam\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Convert RBG to Grey-scale\n",
        "def converter(x):\n",
        "    #x has shape (width, height, channels)\n",
        "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,1:2]) + (0.07 * x[:,:,-1:])\n",
        "\n",
        "penultimate_layer = utils.find_layer_idx(model, 'conv2d_9') #If an error occurs, find the name of layer in the model summary\n",
        "layer_idx = utils.find_layer_idx(model, 'dense_6')\n",
        "\n",
        "for modifier in [None, 'guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 2)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        \n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=0, \n",
        "                              seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "        grey_img = converter(img)\n",
        "\n",
        "        ax[i].imshow(overlay(jet_heatmap, grey_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi9fdsBwIoVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3 Attention heatmap displayed on the origin image"
      ]
    },
    {
      "metadata": {
        "id": "s_r5wBNZ57Wt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for modifier in [None, 'guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 4)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=0, \n",
        "                              seed_input=img, penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "        ax[i].imshow(overlay(jet_heatmap, img))\n",
        "        ax[i + 2].imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}